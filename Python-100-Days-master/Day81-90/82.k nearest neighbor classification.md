## k nearest neighbor classification

$k$ Nearest Neighbor (referred to as kNN, k-Nearest Neighbor) is a simple supervised learning algorithm proposed by Cover and Hart in 1968, which can be used in character recognition, text classification, image recognition and other fields. The working mechanism of kNN is very simple: given a test sample, based on some distance metric (such as Euclidean distance, Manhattan distance, etc.) nearest neighbors‚Äù information to make predictions. For classification tasks, the most frequent class label among the k nearest neighbors can be selected as the predicted result; for regression tasks, the average of the actual outputs (target values) of the k nearest neighbors can be used as the predicted result , of course, a weighted average can also be performed according to the distance, and the closer the distance is, the greater the weight value of the sample.

### Example: Movie Classification Prediction



### k-value selection and cross-checking

The choice of the value of k has a very significant impact on the results of the kNN algorithm. The choice of k value is explained below with the description in Dr. Li Hang's "Statistical Learning Method".

If you choose a smaller value of k, it is equivalent to making predictions with training examples in a smaller neighborhood, the approximation error of "learning" will be reduced, and only training examples that are closer (similar) to the input example will be It works on the prediction result; but the disadvantage is that the estimation error of "learning" will increase, and the prediction result will be very sensitive to the instance points of the neighbors. If the instance points of the neighbors happen to be noise, the prediction will be wrong. In other words, a decrease in the value of $k$ means that the overall model becomes complex and prone to overfitting.

If a larger k value is selected, it is equivalent to predicting with a training instance in a larger neighborhood. The advantage is that the estimation error of learning can be reduced, but the disadvantage is that the approximation error of learning will increase. At this time, training instances that are far (dissimilar) from the input instance will also play a role in the prediction, making the prediction wrong. For the extreme case of $k=N$ (where $N$ represents the number of all training instances), then no matter what the input instance is, it will be predicted to belong to the class with the most training instances. Obviously, such a model completely ignores A large amount of useful information in training examples is not desirable.

In practical applications, the value of $k$ is usually relatively small, and a better value of $k$ can be selected by cross-checking.



### Algorithm advantages and disadvantages

advantage:

1. Simple and effective
2. Low cost of retraining
3. Suitable for class-domain cross-samples
4. Suitable for large sample classification

shortcoming:

1. Lazy learning
2. The interpretability of the output is not strong
3. Not good at handling imbalanced samples
4. The amount of calculation is relatively large