## Introduction to Hive

Hive is a Hadoop-based data warehouse tool open sourced by Facebook. It is currently the most widely used big data processing solution. It can convert SQL queries into MapReduce (a software architecture proposed by Google for parallelization of large-scale data sets. It provides perfect support for SQL and can easily implement big data statistics.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20220210080608.png">

> **Note**: You can learn about the Hadoop ecosystem through <https://www.edureka.co/blog/hadoop-ecosystem>.

If you want to briefly introduce Hive, the following two points are its core:

1. Map the structured data in HDFS into tables.
2. By parsing and converting Hive-SQL, a series of Hadoop-based MapReduce tasks/Spark tasks are finally generated, and data processing is completed by executing these tasks. That is to say, even if you do not learn programming languages ​​such as Java and Scala, you can still process data.

The comparison between Hive and traditional relational databases is shown in the following table.

| | Hive | RDBMS |
| -------- | ----------------- | ------------ |
| Query Language | HQL | SQL |
| Store Data | HDFS | Local File System |
| Execution method | MapReduce / Spark | Executor |
| Execution Delay | High | Low |
| Data Scale | Large | Small |

### Preparation

1. Build a big data platform as shown in the figure below.

    ![bigdata-basic-env](https://gitee.com/jackfrued/mypic/raw/master/20220210080638.png)

2. Access the big data platform through the Client node.

    ![bigdata-vpc](https://gitee.com/jackfrued/mypic/raw/master/20220210080655.png)

3. Create a file system for Hadoop.

    ```Shell
    hadoop fs -mkdir /data
    hadoop fs -chmod g+w /data
    ````

4. Copy the prepared data files to the Hadoop file system.

    ```Shell
    hadoop fs -put /home/ubuntu/data/* /data
    ````

### create/delete database

create.

````SQL
create database if not exists demo;
````

or

```Shell
hive -e "create database demo;"
````

delete.

````SQL
drop database if exists demo;
````

switch.

````SQL
use demo;
````

### type of data

The data types of Hive are shown below.

Basic data types.

| Data Types | Occupied Space | Supported Versions |
| --------- | -------- | -------- |
| tinyint | 1-Byte | |
| smallint | 2-Byte | |
| int | 4-Byte | |
| bigint | 8-Byte | |
| boolean | | |
| float | 4-Byte | |
| double | 8-Byte | |
| string | | |
| binary | | Version 0.8 |
| timestamp | | Version 0.8 |
| decimal | | Version 0.11 |
| char | | Version 0.13 |
| varchar | | Version 0.12 |
| date | | Version 0.12 |

complex data types.

| Data Type | Description | Example |
| -------- | ------------------------ | --------------- ------------------------------ |
| struct | Similar to the structure in C language | `struct<first_name:string, last_name:string>` |
| map | A collection of elements consisting of key-value pairs | `map<string,int>` |
| array | container with variables of the same type | `array<string>` |

### Create and use tables

1. Create an internal table.

    ````SQL
    create table if not exists user_info
    (
    user_id string,
    user_name string,
    sex string,
    age int,
    city ​​string,
    firstactivetime string,
    level int,
    extra1 string,
    extra2 map<string,string>
    )
    row format delimited fields terminated by '\t'
    collection items terminated by ','
    map keys terminated by ':'
    lines terminated by '\n'
    stored as textfile;
    ````

2. Load the data.

    ````SQL
    load data local inpath '/home/ubuntu/data/user_info/user_info.txt' overwrite into table user_info;
    ````

    or

    ````SQL
    load data inpath '/data/user_info/user_info.txt' overwrite into table user_info;
    ````

3. Create a partition table.

    ````SQL
    create table if not exists user_trade
    (
    user_name string,
    piece int,
    price double,
    pay_amount double,
    goods_category string,
    pay_time bigint
    )
    partitioned by (dt string)
    row format delimited fields terminated by '\t';
    ````

4. Set up dynamic partitions.

    ````SQL
    set hive.exec.dynamic.partition=true;
    set hive.exec.dynamic.partition.mode=nonstrict;
    set hive.exec.max.dynamic.partitions=10000;
    set hive.exec.max.dynamic.partitions.pernode=10000;
    ````

5. Copy data (Shell command).

    ```Shell
    hdfs dfs -put /home/ubuntu/data/user_trade/* /user/hive/warehouse/demo.db/user_trade
    ````

6. Repair the partition table.

    ````SQL
    msck repair table user_trade;
    ````

### Inquire

#### Basic syntax

````SQL
select user_name from user_info where city='beijing' and sex='female' limit 10;
select user_name, piece, pay_amount from user_trade where dt='2019-03-24' and goods_category='food';
````

#### group by

````SQL
-- Query how many people bought each category from January to April 2019, and what is the accumulated amount
select goods_category, count(distinct user_name) as user_num, sum(pay_amount) as total from user_trade where dt between '2019-01-01' and '2019-04-30' group by goods_category;
````

````SQL
-- Query users who paid more than 50,000 yuan in April 2019
select user_name, sum(pay_amount) as total from user_trade where dt between '2019-04-01' and '2019-04-30' group by user_name having sum(pay_amount) > 50000;
````

#### order by

````SQL
-- Query the top 5 users who paid the most in April 2019
select user_name, sum(pay_amount) as total from user_trade where dt between '2019-04-01' and '2019-04-30' group by user_name order by total desc limit 5;
````

#### Common functions

1. `from_unixtime`: convert timestamp to date
2. `unix_timestamp`: convert date to timestamp
3. `datediff`: Calculate the time difference between two dates
4. `if`: return different values ​​based on conditions
5. `substr`: String to take substring
6. `get_json_object`: Get the `value` corresponding to the specified `key` from the JSON string, such as: `get_json_object(info, '$.first_name')`.