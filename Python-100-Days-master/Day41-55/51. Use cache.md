## use cache

Usually, the performance bottleneck of web applications occurs on relational databases. When the concurrent access volume is large, if all requests need to complete data persistence operations through relational databases, the database will be overwhelmed. The most important point in optimizing the performance of Web applications is to use cache, and load those data with small data volume but very high access frequency into the cache server in advance. This is a typical method of changing space for time. Usually, the cache server directly places the data in the memory and uses a very efficient data access strategy (hash storage, key-value pair method, etc.), which is far superior to the relational database in read and write performance, so we One of the best options for a web application to connect to a cache server to optimize its performance is to use Redis.

The cache architecture of a web application is roughly as shown in the following figure.

![](res/redis-cache-service.png)

### Django project access to Redis

In the previous course, we have introduced the installation and use of Redis, which will not be repeated here. If you need to access Redis in a Django project, you can use the third-party library `django-redis`, which in turn depends on a third-party library named `redis`, which encapsulates various operations on Redis.

Install `django-redis`.

````Bash
pip install django-redis
````

Modify the cache configuration in the Django configuration file.

````Python
CACHES = {
    'default': {
        # Specify access to Redis service through django-redis
        'BACKEND': 'django_redis.cache.RedisCache',
        # URL of the Redis server
        'LOCATION': ['redis://1.2.3.4:6379/0', ],
        # Prefix for keys in Redis (resolving naming conflicts)
        'KEY_PREFIX': 'vote',
        # other configuration options
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            # Connection pool (preset several spare Redis connections) parameters
            'CONNECTION_POOL_KWARGS': {
                # maximum number of connections
                'max_connections': 512,
            },
            # User password to connect to Redis
            'PASSWORD': 'foobared',
        }
    },
}
````

At this point, our Django project has been able to access Redis. Next, we modify the project code and use the interface written by Redis to obtain subject data to provide caching services.

### Provide caching service for views

#### Declarative caching

The so-called declarative caching refers to adding caching function to the original code through the decorator (proxy) in Python without modifying the original code. For FBV, the code looks like this.

````Python
from django.views.decorators.cache import cache_page


@api_view(('GET', ))
@cache_page(timeout=86400, cache='default')
def show_subjects(request):
    """Get subject data"""
    queryset = Subject.objects.all()
    data = SubjectSerializer(queryset, many=True).data
    return Response({'code': 20000, 'subjects': data})
````

The above code caches the return value (response object) of the view function through the `cache_page` decorator encapsulated by Django. The original intention of `cache_page` is to cache the page rendered by the view function. For the view function that returns JSON data, it is equivalent to caching JSON. data. When using the `cache_page` decorator, you can pass the `timeout` parameter to specify the cache expiration time, and the `cache` parameter to specify which set of cache services to use to cache data. The Django project allows multiple sets of cache services to be configured in the configuration file. The above `cache='default'` specifies the use of the default cache service (because we only configured the cache service named `default` in the previous configuration file) . The return value of the view function will be serialized into a byte string and put into Redis (the str type in Redis can receive byte strings), and the serialization and deserialization of cached data do not need to be handled by ourselves, because the `cache_page` decoration The browser will call the `RedisCache` in the `django-redis` library to connect to Redis, which uses the `DefaultClient` to connect to Redis and uses [pickle serialization](https://python3-cookbook.readthedocs.io/zh_CN /latest/c05/p21_serializing_python_objects.html), `django_redis.serializers.pickle.PickleSerializer` is the default serializer class.

If there is no subject data in the cache, then when accessing subject data through the interface, our view function will issue an SQL statement to the database to obtain the data by executing `Subject.objects.all()`, and the return value of the view function will be cached. Therefore, if the cache is not expired next time the view function is requested, the return value of the view function can be obtained directly from the cache without querying the database again. If you want to know the usage of the cache, you can configure the database log or use Django-Debug-Toolbar to view it. When you access the subject data interface for the first time, you will see the SQL statement for querying subject data. The database issues SQL statements because data can be fetched directly from the cache.

For CBV, you can use the decorator named `method_decorator` in Django to put the decorator of the `cache_page` decorator function on the method in the class, the effect is the same as the above code. It should be reminded that the `cache_page` decorator cannot be placed directly on the class, because it is the decorator of the decorative function, so the Django framework provides `method_decorator` to solve this problem, obviously, `method_decorator` is a Decorator for decorating classes.

````Python
from django.utils.decorators import method_decorator
from django.views.decorators.cache import cache_page


@method_decorator(decorator=cache_page(timeout=86400, cache='default'), name='get')
class SubjectView(ListAPIView):
    """View class for obtaining subject data"""
    queryset = Subject.objects.all()
    serializer_class = SubjectSerializer
````

#### Programmatic caching

The so-called programmatic cache refers to using the cache service through the code written by yourself. Although the amount of code in this method is slightly larger, it is more flexible in the operation and use of the cache than the declarative cache, and is used in actual development. more. The following code removes the `cache_page` decorator used before, and uses the `get_redis_connection` function provided by `django-redis` to directly obtain the Redis connection to operate Redis.

````Python
def show_subjects(request):
    """Get subject data"""
    redis_cli = get_redis_connection()
    # First try to get the subject data from the cache
    data = redis_cli.get('vote:polls:subjects')
    if data:
        # If the subject data is obtained, deserialize it
        data = json.loads(data)
    else:
        # If the subject data is not obtained in the cache, query the database
        queryset = Subject.objects.all()
        data = SubjectSerializer(queryset, many=True).data
        # Serialize the found subject data and put it in the cache
        redis_cli.set('vote:polls:subjects', json.dumps(data), ex=86400)
    return Response({'code': 20000, 'subjects': data})
````

It should be noted that the Django framework provides two ready-made variables `cache` and `caches` to support cache operations. The former accesses the default cache (the cache named `default`), and the latter can be obtained through index operations The specified cache service (eg: `caches['default']`). Sending `get` and `set` messages to the `cache` object can implement read and write operations to the cache, but this method can do limited operations and is not as flexible as the method used in the above code. It is also worth noting that since the Redis connection object obtained by the `get_redis_connection` function can initiate various operations to Redis, including `FLUSHDB`, `SHUTDOWN` and other dangerous operations, so in actual commercial project development, generally Encapsulate `django-redis` again, such as encapsulating a tool class, which only provides the methods for cache operations that the project needs to use, thus avoiding the potential risks of using `get_redis_connection` directly. Of course, you can also use "Read Through" and "Write Through" to update the cache by encapsulating the operation of the cache yourself, which will be introduced below.

### Cache related issues

#### Cache data update

When using a cache, a problem that must be clarified is how to update the data in the cache when the data changes. There are usually the following routines for updating the cache, which are:

1. Cache Aside Pattern
2. Read/Write Through Pattern
3. Write Behind Caching Pattern

The specific method of the first method is to update the database first, and then delete the cache when the data is updated. Note that the method of updating the database first and then updating the cache cannot be used, and the method of deleting the cache and then updating the database cannot be used. You can think for yourself why (consider a scenario with concurrent read and write operations). Of course, the practice of updating the database first and then deleting the cache is also risky in theory, but the probability of problems is extremely low, so many projects use this method.

The first method is equivalent to that the developer who writes the business code is responsible for the operation of the two sets of storage systems (cache and relational database), and the code is very cumbersome to write. The main purpose of the second method is to turn the back-end storage system into a set of codes, and the maintenance of the cache is encapsulated in this set of codes. Among them, Read Through refers to updating the cache in the query operation, that is to say, when the cache is invalid, the cache service is responsible for loading the data, which is transparent to the application side; and Write Through refers to updating the data. If there is no cache hit, update the database directly, and then return. If the cache is hit, the cache is updated, and then the database is updated by the cache service itself (synchronous update). As we said just now, if you encapsulate the Redis operation in the project again, you can implement the "Read Through" and "Write Through" modes. Although this will increase the workload, it is undoubtedly a "once and for all" and " The merit is in the future".

The third way is that when updating data, only the cache is updated, not the database, and the cache service will update the database asynchronously in batches. This approach will greatly improve performance, but at the cost of sacrificing strong data consistency. The implementation logic of the third method is more complicated, because it needs to track which data has been updated, and then refresh it to the persistence layer in batches.

#### Cache penetration

The cache is an intermediate layer added to relieve the pressure on the database. If malicious visitors frequently access data that is not in the cache, the cache will lose its meaning, and all the request pressure will fall on the database in an instant. It causes the database to carry huge pressure and even connection abnormality, similar to the practice of distributed denial of service (DDoS). One way to solve cache penetration is to agree that if the query returns a null value, the null value is also cached, but a short timeout period needs to be set for the cache of this null value. After all, caching such a value is for the cache space. waste. Another way to solve cache penetration is to use Bloom filter, you can understand the specific method by yourself.

#### Cache breakdown

In an actual project, there may be a cached key that expires at a certain point in time, but there are a large number of concurrent requests for the key at this point in time. These requests do not find the data corresponding to the key from the cache, so Data will be obtained directly from the database and written back to the cache. At this time, large concurrent requests may instantly overwhelm the database. This phenomenon is called cache breakdown. The more common solution to cache breakdown is to use a mutex lock. Simply put, when the cache fails, instead of going to the database to load data immediately, first set the mutex lock (for example: setnx in Redis), and only set A request for a successful mutex operation can execute a query to load data from the database and write it into the cache. For other requests that fail to set a mutex, you can perform a short sleep first, and then try to retrieve data from the cache again. If the cache fails If there is no data yet, repeat the operation of setting the mutex just now. The rough reference code is as follows.

````Python
data = redis_cli.get(key)
while not data:
    if redis_cli.setnx('mutex', 'x'):
        redis.expire('mutex', timeout)
        data = db.query(...)
        redis.set(key, data)
        redis.delete('mutex')
    else:
        time.sleep(0.1)
        data = redis_cli.get(key)
````

#### Cache Avalanche

Cache avalanche means that the same expiration time is used when the data is put into the cache, which causes the cache to fail at a certain moment at the same time, and all requests are forwarded to the database, causing the database to collapse due to excessive instantaneous pressure. The solution to the cache avalanche problem is also relatively simple. A random time can be added to the established cache expiration time, which can prevent different keys from collectively invalidating at the same time to a certain extent. Another way is to use multi-level caches. The expiration time of each level of cache is different. In this case, even if a certain level of cache fails collectively, other levels of cache can still provide data and prevent all requests from falling to the database. superior.