## Pandas application-3

### Application of DataFrame

#### Data cleaning

Usually, the data we get from Excel, CSV or database is not very perfect, there may be duplicate values ​​or outliers mixed in due to system or human reasons, and there may be missing values ​​in some fields; moreover, The data in `DataFrame` may also have various problems such as inconsistent formats and dimensions. Therefore, it is especially important to clean the data before starting the data analysis.

##### missing values

Missing values ​​in a data table can be found using the `isnull` or `isna` methods of the `DataFrame` object as shown below.

````Python
emp_df.isnull()
````

or

````Python
emp_df.isna()
````

output:

````
        ename job mgr sal comm dno
eno
1359 False False False False False False
2056 False False False False False False
3088 False False False False False False
3211 False False False False True False
3233 False False False False True False
3244 False False False False True False
3251 False False False False True False
3344 False False False False False False
3577 False False False False True False
3588 False False False False True False
4466 False False False False True False
5234 False False False False True False
5566 False False False False False False
7800 False False True False False False
````

Correspondingly, the `notnull` and `notna` methods can mark non-null values ​​as `True`. If you want to delete these missing values, you can use the `dropna` method of the `DataFrame` object. The `axis` parameter of this method can specify whether to delete along the 0 axis or the 1 axis, that is, when a null value is encountered, whether to delete the entire value The row or the entire column is deleted. The default is to delete along the 0 axis. The code is as follows.

````Python
emp_df.dropna()
````

output:

````
        ename job mgr sal comm dno
eno
1359 Hu Yidao Salesperson 3344.0 1800 200.0 30
2056 Qiao Feng Architect 7800.0 5000 1500.0 20
3088 Li Mochou Designer 2056.0 3500 800.0 20
3344 Huang Rong Sales Supervisor 7800.0 3000 800.0 30
5566 Song Yuanqiao Accountant 7800.0 4000 1000.0 10
````

If you want to delete along the 1 axis, you can use the code below.

````Python
emp_df.dropna(axis=1)
````

output:

````
        ename job sal dno
eno
1359 Hu Yidao Salesperson 1800 30
2056 Qiao Feng Architect 5000 20
3088 Li Mochou Designer 3500 20
3211 Zhang Wuji Programmer 3200 20
3233 Qiu Chuji Programmer 3400 20
3244 Ouyang Feng Programmer 3200 20
3251 Zhang Cuishan Programmer 4000 20
3344 Huang Rong Sales Supervisor 3000 30
3577 Yang Guo Accounting 2200 10
3588 Zhu Jiuzhen Accounting 2500 10
4466 Miao Renfeng Salesperson 2500 30
5234 Guo Jing Cashier 2000 10
5566 Song Yuanqiao Accountant 4000 10
7800 Zhang Sanfeng President 9000 20
````

> **Note**: Many methods of the `DataFrame` object have a parameter called `inplace`, which defaults to `False`, which means that our operation will not modify the original `DataFrame` object, and is to return the processed result through a new `DataFrame` object. If the value of this parameter is set to `True`, then our operation will be directly modified on the original `DataFrame`, and the return value of the method is `None`. Simply put, the above operation does not modify `emp_df`, but returns a new `DataFrame` object.

In some specific scenarios, we can fill empty values. The corresponding method is `fillna`. When filling empty values, you can use the specified value (specified by the `value` parameter), or you can use the previous one in the table. The cell (by setting the parameter `method=ffill`) or the value of the following cell (by setting the parameter `method=bfill`) is filled when the code is as shown below.

````Python
emp_df.fillna(value=0)
````

> **Note**: How to choose the filled value is also a topic worth discussing. In actual work, some statistics (such as mean, mode, etc.) : random interpolation, Lagrangian interpolation, etc.) to fill in, and it is even possible to fill in missing data through regression models, Bayesian models, etc.

output:

````
        ename job mgr sal comm dno
eno
1359 Hu Yidao Salesperson 3344.0 1800 200.0 30
2056 Qiao Feng Analyst 7800.0 5000 1500.0 20
3088 Li Mochou Designer 2056.0 3500 800.0 20
3211 Zhang Wuji Programmer 2056.0 3200 0.0 20
3233 Qiu Chuji Programmer 2056.0 3400 0.0 20
3244 Ouyang Feng Programmer 3088.0 3200 0.0 20
3251 Zhang Cuishan Programmer 2056.0 4000 0.0 20
3344 Huang Rong Sales Supervisor 7800.0 3000 800.0 30
3577 Yang Guo Accounting 5566.0 2200 0.0 10
3588 Zhu Jiuzhen Accounting 5566.0 2500 0.0 10
4466 Miao Renfeng Salesperson 3344.0 2500 0.0 30
5234 Guo Jing Cashier 5566.0 2000 0.0 10
5566 Song Yuanqiao Accountant 7800.0 4000 1000.0 10
7800 Zhang Sanfeng President 0.0 9000 1200.0 20
````

##### Duplicate Values

Next, we first add two rows of data to the previous department table, so that there are two departments named "R&D Department" and "Sales Department" in the department table.

````Python
dept_df.loc[50] = {'dname': 'R&D Department', 'dloc': 'Shanghai'}
dept_df.loc[60] = {'dname': 'Sales Department', 'dloc': 'Changsha'}
dept_df
````

output:

````
    dname dloc
dno
10 Accounting Department Beijing
20 R&D Department Chengdu
30 Sales Department Chongqing
40 Operation and Maintenance Department Tianjin
50 R&D Department Shanghai
60 Sales Department Changsha
````

Now that there are duplicate data in our data table, we can use the `duplicated` method of the `DataFrame` object to determine whether there are duplicate values. This method determines whether the row index is duplicated by default when no parameters are specified. We can also specify whether the row index is duplicated or not. The name `dname` determines whether the department is duplicated, and the code is as follows.

````Python
dept_df.duplicated('dname')
````

output:

````
dno
10 False
20 False
30 False
40 False
50 True
60 True
dtype: bool
````

As can be seen from the above output, the two departments of `50` and `60` are duplicates in terms of department names. If you want to delete duplicate values, you can use the `drop_duplicates` method. The `keep` parameter of this method can be controlled at When a duplicate value is encountered, whether to keep the first item or the last item, or to keep none of the multiple duplicates, delete them all.

````Python
dept_df.drop_duplicates('dname')
````

output:

````
dname dloc
dno
10 Accounting Department Beijing
20 R&D Department Chengdu
30 Sales Department Chongqing
40 Operation and Maintenance Department Tianjin
````

Change the value of the `keep` parameter to `last`.

````Python
dept_df.drop_duplicates('dname', keep='last')
````

output:

````
dname dloc
dno
10 Accounting Department Beijing
40 Operation and Maintenance Department Tianjin
50 R&D Department Shanghai
60 Sales Department Changsha
````

##### Outliers

The full name of outliers in statistics is suspected outliers, also called outliers, and the analysis of outliers is also called outlier analysis. Outliers are "extreme values" that occur in a sample, where data values ​​appear abnormally large or small, and whose distributions deviate significantly from the rest of the observations. In actual work, some outliers may be caused by system or human reasons, but some outliers are not. They can appear repeatedly and stably, and are normal extreme values. For example, the data of top players in many game products are often the same. is the outlier extreme value. Therefore, we can neither ignore the existence of outliers nor simply remove them from data analysis. Paying attention to the occurrence of outliers and analyzing their causes often becomes an opportunity to discover problems and improve decision-making.

The detection of outliers includes Z-score method, IQR method, DBScan clustering, isolation forest, etc. Here we give a brief introduction to the first two methods.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211004192858.png" style="zoom:50%;">

If the data follow a normal distribution, according to the 3σ rule, an outlier is defined as a value that deviates from the mean by more than three standard deviations. Under the normal distribution, the probability of occurrence of values ​​other than 3σ from the mean value is $ P(|x-\mu|>3\sigma)<0.003 $, which is a small probability event. If the data does not obey a normal distribution, it can be described by how many times the standard deviation is away from the mean, where the multiple is the Z-score. The Z-score measures how far a raw score deviates from the mean in units of standard deviation. The formula is as follows.
$$
z = \frac {X - \mu} {\sigma}
$$
The Z-score needs to be determined based on experience and actual conditions. Usually, data points that are more than `3` times away from the standard deviation are regarded as outliers. The following code shows how to detect outliers by the Z-score method.

````Python
import numpy as np


def detect_outliers_zscore(data, threshold=3):
    avg_value = np.mean(data)
    std_value = np.std(data)
    z_score = np.abs((data - avg_value) / std_value)
    return data[z_score > threshold]
````

The IQR (Inter-Quartile Range) in the IQR method represents the interquartile distance, that is, the difference between the upper quartile (Q3) and the lower quartile (Q1). Under normal circumstances, it can be considered that less than $ Q1 - 1.5 \times IQR $ or greater than $ Q3 + 1.5 \times IQR $ is an outlier, and this method of detecting outliers is also a box plot (described later). The default use Methods. The code below shows how to detect outliers by the IQR method.

````Python
import numpy as np


def detect_outliers_iqr(data, whis=1.5):
    q1, q3 = np.quantile(data, [0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - whis * iqr, q3 + whis * iqr
    return data[(data < lower) | (data > upper)]
````

If you want to drop outliers, you can use the `drop` method of the `DataFrame` object, which can drop the specified row or column based on the row index or column index. For example, we think that the monthly salary is lower than `2000` or higher than `8000` is an outlier in the employee table, you can use the following code to delete the corresponding record.

````Python
emp_df.drop(emp_df[(emp_df.sal > 8000) | (emp_df.sal < 2000)].index)
````

If you want to replace the abnormal value, you can do it by assigning a value to the cell, or you can use the `replace` method to replace the specified value. For example, we want to replace the monthly salary of `1800` and `9000` with the average monthly salary, and the subsidy of `800` with `1000`, the code is as follows.

````Python
avg_sal = np.mean(emp_df.sal).astype(int)
emp_df.replace({'sal': [1800, 9000], 'comm': 800}, {'sal': avg_sal, 'comm': 1000})
````

##### Preprocessing

Data preprocessing is also a big topic, which includes operations such as disassembly, transformation, reduction, and discretization of data. Let's first look at the disassembly of the data. If the data in the data table is a time and date, we usually need to disassemble it from dimensions such as year, quarter, month, day, week, hour, minute, etc. If the time and date are represented by strings, you can first pass The `to_datetime` function of `pandas` handles it as a date and time.

In the following example, we first read the Excel file to obtain a set of sales data, the first column of which is the sales date, and we disassemble it into "month", "quarter" and "week", the code is as follows .

````Python
sales_df = pd.read_excel(
    '2020 sales data.xlsx',
    usecols=['Sales Date', 'Sales Area', 'Sales Channel', 'Brand', 'Sales']
)
sales_df.info()
````

> **Note**: If you need the Excel file in the above example, you can get it through the Baidu cloud disk address below. The data is in the "Learn Data Analysis from Scratch" directory. Link: https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g, extraction code: e7b4.

output:

````
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1945 entries, 0 to 1944
Data columns (total 5 columns):
 # Column Non-Null Count Dtype
--- ------ -------------- -----
 0 sale date 1945 non-null datetime64[ns]
 1 sales area 1945 non-null object
 2 Sales channel 1945 non-null object
 3 brands 1945 non-null object
 4 sales 1945 non-null int64
dtypes: datetime64[ns](1), int64(1), object(3)
memory usage: 76.1+ KB
````

````Python
sales_df['month'] = sales_df['sales date'].dt.month
sales_df['quarter'] = sales_df['sales date'].dt.quarter
sales_df['week'] = sales_df['sales date'].dt.weekday
sales_df
````

output:


````
Sales Date Sales Territory Sales Channel Brand Sales Month Quarter Week
0 2020-01-01 Shanghai Pinduoduo Eight Horses 8217 1 1 2
1 2020-01-01 Shanghai Douyin Eight Horses 6351 1 1 2
2 2020-01-01 Shanghai Tmall Eight Horses 14365 1 1 2
3 2020-01-01 Shanghai Tmall Eight Horses 2366 1 1 2
4 2020-01-01 Shanghai Tmall Pippi Shrimp 15189 1 1 2
... ... ... ... ... ... ...
1940 2020-12-30 Beijing Jingdong Flower Girl 6994 12 4 2
1941 2020-12-30 Fujian Entity Eight Horses 7663 12 4 2
1942 2020-12-31 Fujian Entity Flower Girl 14795 12 4 3
1943 2020-12-31 Fujian Douyin Eight Horses 3481 12 4 3
1944 2020-12-31 Fujian Tmall Eight Horses 2673 12 4 3
````

In the above code, through the `dt` property of the `Series` object of the datetime type, get an object to access the datetime, through the `year`, `month`, `quarter`, `hour` and other properties of the object , you can get time information such as year, month, quarter, hour, etc., and what you get is still a `Series` object, which contains a set of time information, so we usually also call this `dt` attribute "date time" vector".

Let's talk about the processing of string type data. We first read the recruitment data of a recruitment website from the specified Excel file.

````Python
jobs_df = pd.read_csv(
    'A recruitment website recruitment data.csv',
    usecols=['city', 'companyFullName', 'positionName', 'salary']
)
jobs_df.info()
````

> **Note**: If you need the Excel file in the above example, you can get it through the Baidu cloud disk address below. The data is in the "Learn Data Analysis from Scratch" directory. Link: https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g, extraction code: e7b4.

output:

````
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3140 entries, 0 to 3139
Data columns (total 4 columns):
 # Column Non-Null Count Dtype
--- ------ -------------- -----
 0 city 3140 non-null object
 1 companyFullName 3140 non-null object
 2 positionName 3140 non-null object
 3 salary 3140 non-null object
dtypes: object(4)
memory usage: 98.2+ KB
````

View the first `5` data.

````Python
jobs_df.head()
````

output:

````
    city ​​companyFullName positionName salary
0 Beijing Dajiang Network Technology (Shanghai) Co., Ltd. Data Analysis Post 15k-30k
1 Beijing Beijing Music Entertainment Time Technology Co., Ltd. Data analysis 10k-18k
2 Beijing Beijing Qianxihe Catering Management Co., Ltd. Data analysis 20k-30k
3 Beijing Jilin Province Haisheng Electronic Commerce Co., Ltd. Data analysis 33k-50k
4 Beijing Webcom Technology (Beijing) Co., Ltd. Data analysis 10k-15k
````

The above data table has a total of `3140` pieces of data, but not all positions are "data analysis" positions. If you want to filter out data analysis positions, you can check whether the `positionName` field contains the key "data analysis". Word, fuzzy matching is required here, how should it be implemented? We can get the `positionName` column first, because the `dtype` of this `Series` object is a string, so you can get the corresponding string vector through the `str` attribute, and then you can use the familiar string method to It operates, the code is shown below.

````Python
jobs_df = jobs_df[jobs_df.positionName.str.contains('Data Analysis')]
jobs_df.shape
````

output:

````
(1515, 4)
````

It can be seen that there are `1515` pieces of filtered data. Next, we also need to process the `salary` field. If we want to count the average salary of all positions or the average salary of each city, we first need to process the salary represented by the range into its median value, the code is as follows.

````Python
jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?')
````

> **Description**: The above code extracts two sets of numbers from the string through regular expression capture groups, corresponding to the lower and upper limits of salary, readers who are not familiar with regular expressions can read my Zhihu column ["Application of Regular Expressions"](https://zhuanlan.zhihu.com/p/158929767) in "Learn Python from Scratch".

output:

````
        0 1
0 15 30
1 10 18
2 20 30
3 33 50
4 10 15
... ...
3065 8 10
3069 6 10
3070 2 4
3071 6 12
3088 8 12
````

It should be reminded that the extracted two columns of data are all string type values. We need to convert them to `int` type to calculate the average value. The corresponding method is the `applymap` method of the `DataFrame` object. The argument to this method is a function that acts on each element in the `DataFrame`. After completing this step, we can use the `apply` method to process the above `DataFrame` into an intermediate value. The parameter of the `apply` method is also a function, which can be applied to the `DataFrame` object by specifying the `axis` parameter. row or column, the code is as follows.

````Python
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
temp_df.apply(np.mean, axis=1)
````

 output:

````
0 22.5
1 14.0
2 25.0
3 41.5
4 12.5
        ...
3065 9.0
3069 8.0
3070 3.0
3071 9.0
3088 10.0
Length: 1515, dtype: float64
````

Next, we can replace the original `salary` column with the above result or add a new column to represent the salary corresponding to the position. The complete code is as follows.

````Python
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
jobs_df['salary'] = temp_df.apply(np.mean, axis=1)
jobs_df.head()
````

output:

````
    city ​​companyFullName positionName salary
0 Beijing Dajiang Network Technology (Shanghai) Co., Ltd. Data Analysis Post 22.5
1 Beijing Beijing Music Entertainment Time Technology Co., Ltd. Data Analysis 14.0
2 Beijing Beijing Qianxihe Catering Management Co., Ltd. Data analysis 25.0
3 Beijing Jilin Province Haisheng Electronic Commerce Co., Ltd. Data analysis 41.5
4 Beijing Webcom Technology (Beijing) Co., Ltd. Data analysis 12.5
````

The `applymap` and `apply` methods are often used in data preprocessing. The `Series` object also has the `apply` method, which is also used for data preprocessing, but the `DataFrame` object also has a method called `transform` The method of ` also transforms the data through the passed in function, similar to the `map` method of the `Series` object. It should be emphasized that the `apply` method has a reduction effect, in short, it can process more data into less data or a piece of data; while the `transform` method has no reduction effect, and can only process data. Transformation, how many pieces of data there are originally, how many pieces of data there are still after processing.

If you want to conduct in-depth analysis and mining of data, non-numeric types such as strings and dates need to be processed into numerical values, because there is no way to calculate correlations for non-numeric types, and there is no way to perform operations such as $\chi^2$ tests. . For string types, they can usually be divided into the following three categories, and then deal with them accordingly.

1. Ordinal Variable: The data represented by the string has an order relationship, so the string can be serialized.
2. Categorical Variable / Nominal Variable: The data represented by the string has no size relationship and level, so it can be processed into a dummy variable (dummy variable) matrix using one-hot encoding.
3. Scale Variable: A string essentially corresponds to a data with a high and low size, and can perform addition and subtraction operations, so you only need to process the string into the corresponding value.

For types 1 and 3, we can use the `apply` or `transform` methods mentioned above, or use the `OrdinalEncoder` in `scikit-learn` to process the type 1 string, which we have in will be covered in subsequent courses. For type 2 strings, you can use the `get_dummies()` function of `pandas` to generate a dummy variable (dummy variable) matrix, as shown in the code below.

````Python
persons_df = pd.DataFrame(
    data={
        'Name': ['Guan Yu', 'Zhang Fei', 'Zhao Yun', 'Ma Chao', 'Huang Zhong'],
        'occupation': ['doctor', 'doctor', 'programmer', 'painter', 'teacher'],
        'Education': ['Graduate', 'College', 'Graduate', 'High School', 'Undergraduate']
    }
)
persons_df
````

output:

````
Name Occupation Education
0 Guan Yu Doctor Graduate
1 Doctor Zhang Fei Junior College
2 Zhao Yun Programmer graduate student
3 Ma Chao painter high school
4 Huang Zhong Teacher Undergraduate
````

Treat occupation as a matrix of dummy variables.

````Python
pd.get_dummies(persons_df['occupation'])
````

output:

````
    Doctor Teacher Painter Programmer
0 1 0 0 0
1 1 0 0 0
2 0 0 0 1
3 0 0 1 0
4 0 1 0 0
````

Process education into values ​​of different sizes.

````Python
def handle_education(x):
    edu_dict = {'High School': 1, 'College': 3, 'Undergraduate': 5, 'Graduate': 10}
    return edu_dict.get(x, 0)


persons_df['Education'].apply(handle_education)
````

output:

````
0 10
1 3
2 10
3 1
4 5
Name: Education, dtype: int64
````

Let's talk about data discretization. Discretization is also called binning. If the value of a variable is a continuous value, then there are countless possibilities for its value, which is very inconvenient when grouping data. At this time, it is very important to discretize continuous variables. . The reason why discretization is called binning is that we can preset some bins, each of which represents the range of data values, so that continuous values ​​can be assigned to different bins to achieve discretization. The following example reads the 2018 Beijing points settlement data. We can group the data according to the settlement points. The specific method is as follows.

````Python
luohu_df = pd.read_csv('data/2018 Beijing Points Settlement Data.csv', index_col='id')
luohu_df.score.describe()
````

output:

````
count 6019.000000
mean 95.654552
std 4.354445
min 90.750000
25% 92.330000
50% 94.460000
75% 97.750000
max 122.590000
Name: score, dtype: float64
````

It can be seen that the maximum value of settlement points is `122.59`, and the minimum value is `90.75`, then we can construct a group of `7` boxes from `90` points to `125` points, every `5` points , the `cut` function of `pandas` can help us first bin the data, the code is as follows.

````Python
bins = np.arange(90, 126, 5)
pd.cut(luohu_df.score, bins, right=False)
````

> **Description**: The default value of the `right` parameter of the `cut` function is `True`, which means that the box is open on the left and closed on the right; modifying it to `False` can make the right border of the box an open interval and the left border a closed interval , you can see the output below.

output:

````
id
1 [120, 125)
2 [120, 125)
3 [115, 120)
4 [115, 120)
5 [115, 120)
           ...
6015 [90, 95)
6016 [90, 95)
6017 [90, 95)
6018 [90, 95)
6019 [90, 95)
Name: score, Length: 6019, dtype: category
Categories (7, interval[int64, left]): [[90, 95) < [95, 100) < [100, 105) < [105, 110) < [110, 115) < [115, 120) < [ 120, 125)]
````

We can group the data according to the results of the binning, and then use the aggregation function to count each group. This is an operation often used in data analysis, which will be introduced in the next chapter. In addition, `pandas` also provides a function called `qcut`, which can specify quantiles to bin the data, and interested readers can study by themselves.