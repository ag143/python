## Concurrent programming in Python-2

In the previous lesson, we said that due to the existence of GIL, multi-threading in CPython cannot take advantage of the multi-core CPU. If you want to break through the limitations of GIL, you can consider using multi-process. For multi-process programs, each process has its own GIL, so multi-process will not be affected by the GIL. So, how should we create and use multiprocessing in a Python program?

###Create process

In Python, processes can be created based on the `Process` class. Although processes and threads are essentially different, the usage of the `Process` class and the `Thread` class is very similar. When using the constructor of the `Process` class to create an object, a function is passed in the `target` parameter to specify the code to be executed by the process, and the `args` and `kwargs` parameters can specify the parameter values ​​used by the function.

````Python
from multiprocessing import Process, current_process
from time import sleep


def sub_task(content, nums):
    # Get the current process object through the current_process function
    # Get the ID number and name of the process through the pid and name attributes of the process object
    print(f'PID: {current_process().pid}')
    print(f'Name: {current_process().name}')
    # Through the following output, it is not difficult to find that each process has its own nums list, and there is no shared memory between processes
    # When the child process is created, the data structure of the parent process is copied, and the values ​​​​of the three processes from pop(0) in the list are all 20
    counter, total = 0, nums.pop(0)
    print(f'Loop count: {total}')
    sleep(0.5)
    while counter < total:
        counter += 1
        print(f'{counter}: {content}')
        sleep(0.01)


def main():
    nums = [20, 30, 40]
    # Create and start a process to execute the specified function
    Process(target=sub_task, args=('Ping', nums)).start()
    Process(target=sub_task, args=('Pong', nums)).start()
    # Execute the sub_task function in the main process
    sub_task('Good', nums)


if __name__ == '__main__':
    main()
````

> **Description**: The above code obtains the current process object through the `current_process` function, and then obtains the process ID through the `pid` attribute of the process object. In Python, the same effect can be achieved using the `getpid` function of the `os` module.

If you like, you can also use the `fork` function of the `os` module to create a process. When this function is called, the operating system automatically copies the current process (parent process) (child process), and the `fork` function of the parent process will Returns the ID of the child process, and the `fork` function in the child process will return `0`, which means that one call to this function will get two different return values ​​in the parent process and the child process. It should be noted that the Windows system does not support the `fork` function. If you are using a Linux or macOS system, you can try the following code.

````Python
import os

print(f'PID: {os.getpid()}')
pid = os.fork()
if pid == 0:
    print(f'Subprocess - PID: {os.getpid()}')
    print('Todo: code executed in child process')
else:
    print(f'parent process - PID: {os.getpid()}')
    print('Todo: code executed in parent process')
````

In short, we still recommend that you create and use multiple processes by directly using the `Process` class, inheriting the `Process` class, and using the process pool (`ProcessPoolExecutor`). These three methods are different from the above. The `fork` function ensures code compatibility and portability. The specific method is similar to the method of creating and using multi-threading mentioned before, and will not be repeated here.

### Comparison of multiprocessing and multithreading

For I/O-intensive tasks such as crawlers, there is no advantage to using multiple processes; but for computing-intensive tasks, multi-processes will significantly improve efficiency compared to multi-threading. We can use the following code to prove it. The following code will determine whether a group of large integers is a prime number through multi-threading and multi-process. Obviously, this is a computationally intensive task. We put the task into multiple threads and multiple processes to speed up the code. , let's see how multithreaded and multiprocessed code behaves differently.

We first implement a multi-threaded version, the code is as follows.

````Python
import concurrent.futures

PRIMES = [
    1116281,
    1297337,
    104395303,
    472882027,
    533000389,
    817504243,
    982451653,
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419
] * 5


def is_prime(n):
    """Determine prime numbers"""
    for i in range(2, int(n ** 0.5) + 1):
        if n % i == 0:
            return False
    return n != 1


def main():
    """Main function"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))


if __name__ == '__main__':
    main()
````

Assuming the above code is saved in a file named `example.py`, on a Linux or macOS system, you can use the `time python example.py` command to execute the program and get OS statistics about execution time, on my macOS , the last line of output of a certain run result is as follows.

````
python example09.py 38.69s user 1.01s system 101% cpu 39.213 total
````

It can be seen from the running results that the multi-threaded code can only make the CPU utilization reach 100%. This has actually proved that the multi-threaded code cannot take advantage of the CPU multi-core feature to speed up the execution of the code. Let's take a look at the multi-process version. We replace the thread pool (`ThreadPoolExecutor`) in the above code with a process pool (`ProcessPoolExecutor`).

Multiprocess version.

````Python
import concurrent.futures

PRIMES = [
    1116281,
    1297337,
    104395303,
    472882027,
    533000389,
    817504243,
    982451653,
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419
] * 5


def is_prime(n):
    """Determine prime numbers"""
    for i in range(2, int(n ** 0.5) + 1):
        if n % i == 0:
            return False
    return n != 1


def main():
    """Main function"""
    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))


if __name__ == '__main__':
    main()
````

> **Tips**: When running the above code, you can check whether multiple Python interpreter processes are started through the operating system's task manager (resource monitor).

We still execute the above code through `time python example.py`, and the last line of the running result is shown below.

````
python example09.py 106.63s user 0.57s system 389% cpu 27.497 total
````

It can be seen that the multi-process version makes the CPU utilization reach nearly 400% on the computer I use, and the CPU time (106.63 seconds) consumed by the user mode when running the code is almost the total code running time ( 27.497 seconds), from both points, I can see that my computer uses a 4-core CPU. Of course, to know how many CPUs or cores your computer has, you can use the following code directly.

````Python
import os

print(os.cpu_count())
````

To sum up, multi-process can break through the limitation of GIL and make full use of the multi-core feature of CPU, which is very important for computationally intensive tasks. Common computing-intensive tasks include scientific computing, image processing, audio and video encoding and decoding, etc. If these computing-intensive tasks themselves can be parallelized, then using multiple processes should be a better choice.

### Interprocess communication

Before explaining the inter-process communication, let me give you a task: start two processes, one outputs "Ping", one outputs "Pong", when the "Ping" and "Pong" output by the two processes add up to a total of 50, to end the program. Sounds very simple, but when actually writing the code, since multiple processes cannot exchange data directly through shared memory like multiple threads, the following code cannot achieve the result we want. .

````Python
from multiprocessing import Process
from time import sleep

counter = 0


def sub_task(string):
    global counter
    while counter < 50:
        print(string, end='', flush=True)
        counter += 1
        sleep(0.01)

        
def main():
    Process(target=sub_task, args=('Ping', )).start()
    Process(target=sub_task, args=('Pong', )).start()


if __name__ == '__main__':
    main()
````


The above code looks fine, but the final result is that "Ping" and "Pong" output 50 each. Remind you again that when we create a process in a program, the child process will copy the parent process and all its data structures. Each child process has its own independent memory space, which means that each of the two child processes has one ` The counter variable, they are all incremented from `0` to `50`, so the result is predictable. A simpler way to solve this problem is to use the `Queue` class in the `multiprocessing` module, which is a queue that can be shared by multiple processes. The bottom layer is implemented through the underlying pipe and semaphore mechanism of the operating system. , the code is shown below.

````Python
import time
from multiprocessing import Process, Queue


def sub_task(content, queue):
    counter = queue.get()
    while counter < 50:
        print(content, end='', flush=True)
        counter += 1
        queue.put(counter)
        time.sleep(0.01)
        counter = queue.get()


def main():
    queue = Queue()
    queue.put(0)
    p1 = Process(target=sub_task, args=('Ping', queue))
    p1.start()
    p2 = Process(target=sub_task, args=('Pong', queue))
    p2.start()
    while p1.is_alive() and p2.is_alive():
        pass
    queue.put(50)


if __name__ == '__main__':
    main()
````

> **Tip**: The `get` method of the `multiprocessing.Queue` object will block by default when the queue is empty, and will not return until data is obtained. If you do not want the method to block and you need to specify the blocking timeout, you can specify the `block` and `timeout` parameters.

The above code allows three processes (`p1`, `p2` and the main process) to achieve data sharing through the `get` and `put` methods of the `Queue` class, which is the so-called inter-process communication. In this way, when the value retrieved from `Queue` is greater than or equal to `50`, `p1` and `p2` will jump out of the `while` loop, thereby terminating the execution of the process. The loop on line 22 of the code is to wait for one of the two processes `p1` and `p2` to end. At this time, the main process also needs to put a value greater than or equal to `50` into `Queue`, so that the other has not ended The process will also be terminated for reading this value greater than or equal to `50`.

There are many ways to communicate between processes. For example, sockets can also be used to communicate between two processes, even if the two processes are not on the same host. Interested readers can understand by themselves.

### A brief summary

In Python, we can also create subprocesses by executing other commands through the `call` function of the `subprocess` module, which is equivalent to calling other programs in our program. We will not discuss these knowledge here for the time being. Interested readers You can do your own research.

For Python developers, consider using multithreading in the following situations:

1. The program needs to maintain many shared states (especially mutable states). Lists, dictionaries, and sets in Python are thread-safe (multiple threads operate on the same list, dictionary, or set at the same time, without causing errors and data.) problem), so using threads instead of processes is relatively inexpensive to maintain shared state.
2. The program will spend a lot of time on I/O operations, there is not much need for parallel computing and it does not need to take up too much memory.

Then you should consider using multiprocessing in the following situations:

1. The program performs computationally intensive tasks (eg: audio and video codec, data compression, scientific computing, etc.).
2. The input of the program can be divided into blocks in parallel, and the results of the operations can be combined.
3. The program has no restrictions on memory usage and is not strongly dependent on I/O operations (such as reading and writing files, sockets, etc.).