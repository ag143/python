## Introduction to the crawler framework Scrapy

When you write a lot of crawler programs, you will find that every time you write a crawler program, you need to implement page acquisition, page parsing, crawler scheduling, exception handling, and anti-crawling from beginning to end. There are many The work is actually simple and tedious repetitive work. So, is there any way to improve the efficiency of our crawler code writing? The answer is yes, that is to use the crawler framework, and among all the crawler frameworks, Scrapy should be the most popular and powerful framework.

### Scrapy Overview

Scrapy is a very popular web crawling framework based on Python that can be used to crawl web sites and extract structured data from pages. The following figure shows the basic architecture of Scrapy, which includes the main components and the data processing flow of the system (red arrows with numbers in the figure).

![](https://gitee.com/jackfrued/mypic/raw/master/20210824003638.png)

#### Scrapy Components

Let's start with the components in Scrapy.

1. Scrapy Engine: The data processing process used to control the entire system.
2. Scheduler: The scheduler accepts requests from the engine, sorts them into the queue, and returns them to the engine after the request is made.
3. Downloader (Downloader): The main responsibility of the downloader is to crawl the web page and return the content of the web page to the spiders (Spiders).
4. Spiders: Spiders are user-defined classes that parse web pages and crawl specific URLs. Each spider can handle a domain name or a group of domain names. A module for fetching and parsing rules.
5. Item Pipeline: The main responsibility of the pipeline is to process data items extracted from web pages by spiders. Its main task is to clean, validate and store data. When the page is parsed by the spider, it is sent to the data pipeline, and the data is processed in several specific orders. Each data pipeline component is a Python class that takes a data item and executes methods to process the data item, and also needs to determine whether it needs to continue to the next step in the data pipeline or simply discard it. Data pipelines typically perform tasks such as cleaning the HTML data, validating the parsed data (checking that the entry contains the necessary fields), checking for duplicate data (discarding if duplicates), storing the parsed data in a database (relational database or NoSQL database).
6. Middlewares: Middleware is a hook framework between the engine and other components, mainly to provide custom code to expand the functions of Scrapy, including downloader middleware and spider middleware.

#### Data processing flow

The entire data processing process of Scrapy is controlled by the engine. The usual operation process includes the following steps:

1. The engine asks the spider which website it needs to process and asks the spider to hand it the first URL to process.

2. The engine asks the scheduler to put the URLs that need to be processed in a queue.

3. The engine gets the next page to crawl from the scheduler.

4. The scheduler returns the next crawled URL to the engine, and the engine sends it to the downloader through the download middleware.

5. When the web page is downloaded by the downloader, the response content is sent to the engine through the download middleware; if the download fails, the engine will notify the scheduler to record the URL and download it again later.

6. The engine receives the response from the downloader and sends it to the spider via the spider middleware for processing.

7. The spider processes the response and returns the crawled data items, in addition to sending new URLs to the engine that need to be followed up.

8. The engine sends the captured data entry into the data pipeline, and sends the new URL to the scheduler and puts it in the queue.

Steps 2 to 8 in the above operations will be repeated until there is no URL to request in the scheduler, and the crawler will stop working.

### Install and use Scrapy

Scrapy can be installed using the Python package management tool `pip`.

```Shell
pip install scrapy
````

Use the `scrapy` command on the command line to create a project named `demo`.

````Bash
scrapy startproject demo
````

The directory structure of the project is shown in the following figure.

```Shell
demo
|_____ demo
|________ spiders
|____________ __init__.py
|________ __init__.py
|________ items.py
|________ middlewares.py
|________ pipelines.py
|________ settings.py
|_____ scrapy.cfg
````

Change to the `demo` directory and create a spider called `douban` with the following command.

````Bash
scrapy genspider douban movie.douban.com
````

#### A simple example

Next, we implement a crawler that crawls Douban movies Top250 movie titles, ratings and golden sentences.

1. Define fields in the `Item` class of `items.py`. These fields are used to save data and facilitate subsequent operations.

   ````Python
   import scrapy
   
   
   class DoubanItem(scrapy.Item):
       title = scrapy.Field()
       score = scrapy.Field()
       motto = scrapy.Field()
   ````
   
2. Modify the file named `douban.py` in the `spiders` folder, which is the core of the spider program and requires us to add the code for parsing the page. Here, we can get the movie information by parsing the `Response` object, the code is as follows.

   ````Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
       start_urls = ['https://movie.douban.com/top250?start=0&filter=']
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   ````
   It is not difficult to see from the above code that we can use CSS selectors for page parsing. Of course, if you want, you can also use XPath or regular expressions for page parsing, the corresponding methods are `xpath` and `re` respectively.

   To generate subsequent crawling requests, we can use `yield` to produce a `Request` object. The `Request` object has two very important properties, one is `url`, which represents the address to be requested; the other is `callback`, which represents the callback function to be executed after getting the response. We can slightly modify the above code.
   
```Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
       start_urls = ['https://movie.douban.com/top250?start=0&filter=']
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   
           hrefs = sel.css('#content > div > div.article > div.paginator > a::attr("href")')
           for href in hrefs:
               full_url = response.urljoin(href.extract())
               yield Request(url=full_url)
   ```

   到这里，我们已经可以通过下面的命令让爬虫运转起来。

   ```Shell
   scrapy crawl movie
   ```

   可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过`-o`参数来指定文件名，Scrapy 支持我们将爬取到的数据导出成 JSON、CSV、XML 等格式。

   ```Shell
   scrapy crawl moive -o result.json
   ```

   不知大家是否注意到，通过运行爬虫获得的 JSON 文件中有`275`条数据，那是因为首页被重复爬取了。要解决这个问题，可以对上面的代码稍作调整，不在`parse`方法中解析获取新页面的 URL，而是通过`start_requests`方法提前准备好待爬取页面的 URL，调整后的代码如下所示。

   ```Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
   
       def start_requests(self):
           for page in range(10):
               yield Request(url=f'https://movie.douban.com/top250?start={page * 25}')
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   ```

3. 如果希望完成爬虫数据的持久化，可以在数据管道中处理蜘蛛程序产生的`Item`对象。例如，我们可以通过前面讲到的`openpyxl`操作 Excel 文件，将数据写入 Excel 文件中，代码如下所示。

   ```Python
   import openpyxl
   
   from demo.items import MovieItem
   
   
   class MovieItemPipeline:
   
       def __init__(self):
           self.wb = openpyxl.Workbook()
           self.sheet = self.wb.active
           self.sheet.title = 'Top250'
           self.sheet.append(('名称', '评分', '名言'))
   
       def process_item(self, item: MovieItem, spider):
           self.sheet.append((item['title'], item['score'], item['motto']))
           return item
   
       def close_spider(self, spider):
           self.wb.save('豆瓣电影数据.xlsx')
   ```

   上面的`process_item`和`close_spider`都是回调方法（钩子函数）， 简单的说就是 Scrapy 框架会自动去调用的方法。当蜘蛛程序产生一个`Item`对象交给引擎时，引擎会将该`Item`对象交给数据管道，这时我们配置好的数据管道的`parse_item`方法就会被执行，所以我们可以在该方法中获取数据并完成数据的持久化操作。另一个方法`close_spider`是在爬虫结束运行前会自动执行的方法，在上面的代码中，我们在这个地方进行了保存 Excel 文件的操作，相信这段代码大家是很容易读懂的。

   总而言之，数据管道可以帮助我们完成以下操作：

   - 清理 HTML 数据，验证爬取的数据。
   - 丢弃重复的不必要的内容。
   - 将爬取的结果进行持久化操作。

4. 修改`settings.py`文件对项目进行配置，主要需要修改以下几个配置。

   ```Python
   # 用户浏览器
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
   
   # 并发请求数量 
   CONCURRENT_REQUESTS = 4
   
   # 下载延迟
   DOWNLOAD_DELAY = 3
   # 随机化下载延迟
   RANDOMIZE_DOWNLOAD_DELAY = True
   
   # 是否遵守爬虫协议
   ROBOTSTXT_OBEY = True
   
   # 配置数据管道
   ITEM_PIPELINES = {
      'demo.pipelines.MovieItemPipeline': 300,
   }
   ```

   > **说明**：上面配置文件中的`ITEM_PIPELINES`选项是一个字典，可以配置多个处理数据的管道，后面的数字代表了执行的优先级，数字小的先执行。