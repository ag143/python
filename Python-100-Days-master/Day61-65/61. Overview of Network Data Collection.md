## Network Data Collection Overview

A crawler, also often called a web spider, is a robot program (automated script code) that automatically browses websites according to certain rules and obtains the required information. It is widely used in Internet search engines and data collection. Anyone who has used the Internet and browsers knows that in addition to the text information for users to read, web pages also contain some hyperlinks. It is through the hyperlink information in web pages that web crawlers continuously obtain the addresses of other pages on the network. Data collection is then continued. Because of this, the process of network data collection is like a crawler or spider roaming on the network, so it is called a crawler or a web spider.

### Application areas of crawler

Ideally, all ICPs (Internet Content Providers) should provide APIs for their websites to share the data they allow other programs to fetch, in which case crawlers are not needed at all. Well-known domestic e-commerce platforms (such as Taobao, JD.com, etc.) and social platforms (such as Weibo, WeChat, etc.) all provide their own API interfaces, but such API interfaces usually have an impact on the data that can be captured and the capture The frequency of data is limited. For most companies, timely access to industry data and competitive data is one of the important aspects of business survival. However, for most companies, data is their inherent shortcoming. In this case, it is very important for these enterprises to reasonably use crawlers to obtain data and extract information of commercial value from it.

The application fields of crawlers are actually very wide, and we list some of them below. Interested readers can explore the relevant content by themselves.

1. Search Engines
2. News Aggregation
3. Social Apps
4. Public opinion monitoring
5. Industry data

### Discussion on the legality of reptiles

I often hear people say that "the reptile is well written, and you can eat all you can eat in prison", so is it illegal to program a crawler program? We can interpret this issue from the following perspectives.

1. The field of web crawler is still in the pioneering stage. Although the Internet world has established a certain moral code through its own game rules, namely the Robots protocol (the full name is "web crawler exclusion standard"), the legal part is still establishing and In the process of improvement, that is to say, this field is still a gray area for the time being.
2. "It is a license not prohibited by law". If the crawler, like a browser, obtains the data displayed on the front end (public information on the web page) instead of the private and sensitive information in the background of the website, it is not too worried about the constraints of laws and regulations. Because the current development speed of the big data industry chain far exceeds the perfection of the law.
3. When crawling a website, you need to restrict your crawler to abide by the Robots protocol, and at the same time control the speed at which the web crawler crawls data; when using data, you must respect the intellectual property rights of the website (starting from the Web 2.0 era, Although a lot of data on the Web is provided by users, the website platform has invested operating costs. When users register and publish content, the platform usually has obtained the ownership, use and distribution rights of the data). If these rules are violated, the chances of losing a lawsuit are quite high.
4. Properly concealing your identity is necessary when writing crawler programs, and it is best not to be proved by the other party that your crawler has destroyed other people's movable properties (such as servers).
5. Don't open source or display your crawler code on the public network (such as a code hosting platform), these behaviors usually bring unnecessary trouble to yourself.

#### Robots Protocol

Most websites will define a `robots.txt` file, which is a gentleman's agreement and not a game rule that all crawlers must follow. Take Taobao's [`robots.txt`](http://www.taobao.com/robots.txt) file as an example to see what restrictions Taobao has on crawlers.

````
User-agent: Baiduspider
Disallow: /

User-agent: baiduspider
Disallow: /
````

As can be seen from the above file, Taobao prohibits Baidu crawlers from crawling any of its resources, so when you search for "Taobao" on Baidu, the search result will appear below: "Because the `robots.txt` file of this website has a restriction instruction (restrict search engine crawling), the system cannot provide the content description of this page". As a search engine, Baidu complies with Taobao's `robots.txt` protocol at least on the surface, so users cannot search Taobao's internal product information from Baidu.

Figure 1. Baidu search results of Taobao

![](https://gitee.com/jackfrued/mypic/raw/master/20210824004320.png)

The following is the [`robots.txt`](https://www.douban.com/robots.txt) file of Douban.com, you can interpret it yourself to see what kind of restrictions it has made.

````
User-agent: *
Disallow: /subject_search
Disallow: /amazon_search
Disallow: /search
Disallow: /group/search
Disallow: /event/search
Disallow: /celebrities/search
Disallow: /location/drama/search
Disallow: /forum/
Disallow: /new_subject
Disallow: /service/iframe
Disallow: /j/
Disallow: /link2/
Disallow: /recommend/
Disallow: /doubanapp/card
Disallow: /update/topic/
Disallow: /share/
Allow: /ads.txt
Sitemap: https://www.douban.com/sitemap_index.xml
Sitemap: https://www.douban.com/sitemap_updated_index.xml
# Crawl-delay: 5

User-agent: Wandoujia Spider
Disallow: /

User-agent: Mediapartners-Google
Disallow: /subject_search
Disallow: /amazon_search
Disallow: /search
Disallow: /group/search
Disallow: /event/search
Disallow: /celebrities/search
Disallow: /location/drama/search
Disallow: /j/
````

### Hypertext Transfer Protocol (HTTP)

Before we start explaining crawlers, let's review a little bit about the Hypertext Transfer Protocol (HTTP), because what we see on a web page is usually the result of a browser executing HTML (Hypertext Markup Language), and HTTP is the transmission of HTML data protocol. Like many other application-level protocols, HTTP is built on top of TCP (Transmission Control Protocol). It utilizes the reliable transmission services provided by TCP to realize data exchange in Web applications. According to the introduction on Wikipedia, the original purpose of designing HTTP is to provide a way to publish and receive [HTML](https://zh.wikipedia.org/wiki/HTML) pages, that is, this protocol is to browse The carrier of data transmitted between the server and the web server. For detailed information about HTTP and its current development status, you can read ["Introduction to HTTP Protocol"](http://www.ruanyifeng.com/blog/2016/08/http.html), ["Introduction to Internet Protocol"] (http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html), ["Illustration HTTPS Protocol"](http://www.ruanyifeng.com/blog/2014/09/illustration-ssl. html) and other articles to understand.

The picture below shows the HTTP request and response packets (protocol data) when I used the open source protocol analysis tool Ethereal (the predecessor of WireShark) when I was working in the Sichuan Provincial Key Laboratory of Network Communication Technology. Through the data of the network adapter, the protocol data from the physical link layer to the application layer can be clearly seen.

Figure 2. HTTP request

![http-request](https://gitee.com/jackfrued/mypic/raw/master/20210824003915.png)

An HTTP request is usually composed of four parts: a request line, a request header, a blank line, and a message body. If no data is sent to the server, the message body is not a necessary part. The request line includes the request method (GET, POST, etc., as shown in the following table), resource path and protocol version; the request header is composed of several key-value pairs, including browser, encoding method, preferred language, caching strategy and other information; The request header is followed by a blank line and the message body.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20210825002720.PNG" width="65%">

Figure 3. HTTP response

![http-response](https://gitee.com/jackfrued/mypic/raw/master/20210824234158.png)

An HTTP response is usually composed of four parts: a response line, a response header, a blank line, and a message body. The message body is the data of the service response, which may be an HTML page, or JSON or binary data. The response line contains the protocol version and response status code. There are many response status codes, and the common ones are shown in the following table.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20210825002802.PNG" width="65%">

#### Related tools

Let's first introduce some auxiliary tools for developing crawler programs. These tools are believed to help you do more with less.

1. Chrome Developer Tools: Google Chrome's built-in developer tools. The most commonly used functional modules of this tool are:

   - Elements: Used to view or modify HTML element attributes, CSS properties, monitor events, etc. CSS can be modified and displayed in real time, which greatly facilitates developers to debug pages.
   - Console: used to execute one-time code, view JavaScript objects, view debug log information or exception information. The console is actually an interactive environment for executing JavaScript code.
   - Source code (Sources): used to view the HTML file source code, JavaScript source code, CSS source code of the page, and most importantly, you can debug the JavaScript source code, you can add breakpoints and single-step execution to the code.
   - Network: Used for HTTP requests, HTTP responses, and information related to network connections.
   - Application: used to view browser local storage, background tasks, etc. Local storage mainly includes cookies, local storage, session storage, etc.

   ![chrome-developer-tools](https://gitee.com/jackfrued/mypic/raw/master/20210824004034.png)
   
2. Postman: A powerful web debugging and RESTful request tool. Postman can help us simulate requests, customize our requests and view server responses very easily.

   ![postman](https://gitee.com/jackfrued/mypic/raw/master/20210824004048.png)

3. HTTPie: Command line HTTP client.

   Install.

   ````Bash
   pip install httpie
   ````

   use.

   ````Bash
   http --header http --header https://movie.douban.com/
   
   HTTP/1.1 200 OK
   Connection: keep-alive
   Content-Encoding: gzip
   Content-Type: text/html; charset=utf-8
   Date: Tue, 24 Aug 2021 16:48:00 GMT
   Keep-Alive: timeout=30
   Server: dae
   Set-Cookie: bid=58h4BdKC9lM; Expires=Wed, 24-Aug-22 16:48:00 GMT; Domain=.douban.com; Path=/
   Strict-Transport-Security: max-age=15552000
   Transfer-Encoding: chunked
   X-Content-Type-Options: nosniff
   X-DOUBAN-NEWBID: 58h4BdKC9lM
   ````

4. `builtwith` library: tools for identifying technologies used by websites.

   Install.

   ````Bash
   pip install builtwith
   ````

   use.

   ````Python
   import ssl
   
   import builtwith
   
   ssl._create_default_https_context = ssl._create_unverified_context
   print(builtwith.parse('http://www.bootcss.com/'))
   ````

5. The `python-whois` library: a tool for querying website owners.

   Install.

   ````Bash
   pip3 install python-whois
   ````

   use.

   ````Python
   import whois
   
   print(whois.whois('https://www.bootcss.com'))
   ````

### Basic workflow of crawler

A basic crawler is usually divided into three parts: data collection (web page download), data processing (web page parsing), and data storage (persistent useful information). Of course, more advanced crawlers will collect and process data. Using concurrent programming or distributed technology, this requires the participation of schedulers (arranging threads or processes to perform corresponding tasks), background management programs (monitoring the working status of crawlers and checking the results of data capture), etc.

![crawler-workflow](https://gitee.com/jackfrued/mypic/raw/master/20210824004107.png)

Generally speaking, the workflow of a crawler includes the following steps:

1. Set a crawl target (seed page/start page) and fetch the web page.
2. When the server is unreachable, try to download the page again according to the specified number of retries.
3. Set the user agent or hide the real IP when needed, otherwise the page may not be accessible.
4. Perform necessary decoding operations on the obtained page and then capture the required information.
5. Extract the link information in the page through a certain method (such as regular expression) in the obtained page.
6. Do further processing on the link (fetch the page and repeat the above).
7. Persist useful information for subsequent processing.
