## Application of concurrent programming in crawler

In the previous course, we have introduced multi-threading, multi-process and asynchronous programming in Python. Through these three means, we can achieve concurrent or parallel programming, which can speed up the execution of code on the one hand, and on the other hand. Bring a better user experience. Crawler programs are typically I/O-intensive tasks. For I/O-intensive tasks, both multithreading and asynchronous I/O are good choices, because when a part of the program is blocked by an I/O operation , the rest of the program can still run, so we don't waste a lot of time waiting and blocking. Let's take the example of crawling the "[360 Pictures](https://image.so.com/)" website and saving it locally, to show you how to program using single-threaded, multi-threaded and asynchronous I/O. What is the difference between crawlers and a simple comparison of their execution efficiency.

The pages of the "360 Pictures" website use the [Ajax](https://developer.mozilla.org/zh-CN/docs/Web/Guide/AJAX) technology, which is an asynchronous loading of data and The technique of partially refreshing the page. Simply put, the pictures on the page are generated by asynchronously obtaining JSON data through JavaScript code and rendering them dynamically, and the entire page also uses waterfall loading (while scrolling down, more pictures are loaded). We can find the data interface that provides dynamic content in the "Developer Tools" of the browser, as shown in the figure below, the image information we need is in the JSON data returned by the server.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211205221352.png" style="zoom:50%;">

For example, to get the picture of the "beauty" channel, we can request the URL shown below, where the parameter `ch` represents the requested channel, the parameter value `beauty` after `=` represents the "beauty" channel, and the parameter ` sn` is equivalent to the page number, `0` means the first page (a total of `30` pictures), `30` means the second page, `60` means the third page, and so on.

````
https://image.so.com/zjl?ch=beauty&sn=0
````

### Single thread version

Download a total of `90` images from the "Beauty" channel via the URL above.

````Python
"""
example04.py - single-threaded version of the crawler
"""
import os

import requests


def download_picture(url):
    filename = url[url.rfind('/') + 1:]
    resp = requests.get(url)
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file:
            file.write(resp.content)


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    for page in range(3):
        resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}')
        if resp.status_code == 200:
            pic_dict_list = resp.json()['list']
            for pic_dict in pic_dict_list:
                download_picture(pic_dict['qhimg_url'])

if __name__ == '__main__':
    main()
````

On macOS or Linux systems, we can use the `time` command to know the execution time of the above code and the CPU utilization as shown below.

````Bash
time python3 example04.py
````

Below is the result of executing the single-threaded crawler code on my computer.

````
python3 example04.py 2.36s user 0.39s system 12% cpu 21.578 total
````

Here we only need to pay attention to the total time taken by the code to be `21.578` seconds and the CPU utilization to be `12%`.

### Multithreaded version

We modified the above code to a multi-threaded version using the thread pool technique we talked about earlier.

````Python
"""
example05.py - multithreaded version of the crawler
"""
import os
from concurrent.futures import ThreadPoolExecutor

import requests


def download_picture(url):
    filename = url[url.rfind('/') + 1:]
    resp = requests.get(url)
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file:
            file.write(resp.content)


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    with ThreadPoolExecutor(max_workers=16) as pool:
        for page in range(3):
            resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}')
            if resp.status_code == 200:
                pic_dict_list = resp.json()['list']
                for pic_dict in pic_dict_list:
                    pool.submit(download_picture, pic_dict['qhimg_url'])


if __name__ == '__main__':
    main()
````

Execute the command shown below.

````Bash
time python3 example05.py
````

The execution result of the code is as follows:

````
python3 example05.py 2.65s user 0.40s system 95% cpu 3.193 total
````

### Asynchronous I/O version

We use `aiohttp` to modify the above code to a version for asynchronous I/O. In order to realize the acquisition of network resources and write file operations in the way of asynchronous I/O, we first have to install the three-party libraries `aiohttp` and `aiofile`, the commands are as follows.

````Bash
pip install aiohttp aiofile
````

The usage of `aiohttp` has been briefly introduced in the previous course. The `async_open` function in the `aiofile` module is used in much the same way as the Python built-in function `open`, except that it supports asynchronous operations. Below is the crawler code for the asynchronous I/O version.

````Python
"""
example06.py - Asynchronous I/O version crawler
"""
import asyncio
import json
import os

import aiofile
import aiohttp


async def download_picture(session, url):
    filename = url[url.rfind('/') + 1:]
    async with session.get(url, ssl=False) as resp:
        if resp.status == 200:
            data = await resp.read()
            async with aiofile.async_open(f'images/beauty/{filename}', 'wb') as file:
                await file.write(data)


async def fetch_json():
    async with aiohttp.ClientSession() as session:
        for page in range(3):
            async with session.get(
                url=f'https://image.so.com/zjl?ch=beauty&sn={page * 30}',
                ssl=False
            ) as resp:
                if resp.status == 200:
                    json_str = await resp.text()
                    result = json.loads(json_str)
                    for pic_dict in result['list']:
                        await download_picture(session, pic_dict['qhimg_url'])


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    loop = asyncio.get_event_loop()
    loop.run_until_complete(fetch_json())
    loop.close()


if __name__ == '__main__':
    main()
````

Execute the command shown below.

````Bash
time python3 example06.py
````

The execution result of the code is as follows:

````
python3 example06.py 0.82s user 0.21s system 27% cpu 3.782 total
````

### Summarize

By comparing the execution results of the above three pieces of code, we can draw a conclusion that using both multithreading and asynchronous I/O can improve the performance of the crawler, because we don't need to waste time waiting and blocking caused by I/O operations , and the execution result of the `time` command also tells us that the CPU utilization of the single-threaded code is only `12%`, while the CPU utilization of the multi-threaded version is as high as `95%`; the execution time of the single-threaded version of the crawler About `21` seconds, while the multithreaded and asynchronous I/O version only took `3` seconds. In addition, multithreaded code consumes more CPU resources than asynchronous I/O code when the runtime difference is not large, because multithreaded scheduling and switching also consume CPU time. So far, the advantages and disadvantages of the three methods on I/O-intensive tasks are clear at a glance. Of course, this is only the result of running on my computer. If the network conditions are not ideal or the response of the target website is very slow, the advantages of using multi-threading and asynchronous I/O will be more obvious, and interested readers can experiment by themselves.