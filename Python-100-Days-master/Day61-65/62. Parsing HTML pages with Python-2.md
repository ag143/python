## Parsing HTML pages with Python

In the previous course, we talked about using the `request` tripartite library to obtain network resources, and also introduced some basic knowledge of front-end. Next, let's explore how to parse the HTML code to extract useful information from the page. Before, we tried to extract page content with regular expression capturing group operation, but writing a correct regular expression is also a headache. In order to solve this problem, we must first have a deep understanding of the structure of HTML pages, and then study other methods of parsing pages on this basis.

### Structure of the HTML page

We open any website in the browser, and then through the right-click menu, select the "Show Webpage Source Code" menu item, we can see the HTML code corresponding to the webpage.

![image-20210822094218269](https://gitee.com/jackfrued/mypic/raw/master/20210822094218.png)

The `1` line of the code is the document type declaration, the `<html>` tag on line `2` is the start tag of the root tag of the entire page, and the last line is the end tag of the root tag `</html>`. There are two sub-tags `<head>` and `<body>` under the `<html>` tag. The content placed under the `<body>` tag will be displayed in the browser window, and this part of the content is the main body of the web page; The content placed under the `<head>` tag is not displayed in the browser window, but it contains important meta information about the page, usually called the head of the page. The general code structure of the HTML page is shown below.

````HTML
<!doctype html>
<html>
    <head>
        <!-- Meta information of the page, such as character encoding, title, keywords, media queries, etc. -->
    </head>
    <body>
        <!-- The body of the page, the content displayed in the browser window -->
    </body>
</html>
````

Tags, Cascading Style Sheets (CSS), and JavaScript are the three elements that make up an HTML page. Tags are used to carry the content to be displayed on the page, CSS is responsible for rendering the page, and JavaScript is used to control the interactive behavior of the page. To parse HTML pages, you can use XPath syntax, which is originally a query syntax for XML, which can extract the content or tag attributes in tags according to the hierarchical structure of HTML tags; in addition, you can also use CSS selectors to locate pages elements, just like rendering page elements with CSS.

### XPath parsing

XPath is a syntax for finding information in XML (eXtensible Markup Language) documents. XML, similar to HTML, is also a tag language that uses tags to carry data. The difference is that XML tags are extensible and customizable. And XML has stricter syntax requirements. XPath uses path expressions to select nodes or node sets in an XML document, where nodes include elements, attributes, text, namespaces, processing instructions, comments, root nodes, and so on. Below we use an example to illustrate how to use XPath to parse the page.

````XML
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
    <book>
      <title lang="eng">Harry Potter</title>
      <price>29.99</price>
    </book>
    <book>
      <title lang="en">Learning XML</title>
      <price>39.95</price>
    </book>
</bookstore>
````

For the above XML file, we can get the nodes in the document using the XPath syntax as shown below.

| path expression | result |
| --------------- | --------------------------------- --------------------------- |
| `/bookstore` | Selects the root element bookstore. **Note**: If a path starts with a forward slash ( / ), the path always represents an absolute path to an element! |
| `//book` | Selects all book child elements, regardless of their position in the document. |
| `//@lang` | Selects all attributes named lang. |
| `/bookstore/book[1]` | Selects the first book element that is a child of the bookstore. |
| `/bookstore/book[last()]` | Selects the last book element that is a child of bookstore. |
| `/bookstore/book[last()-1]` | Selects the second-to-last book element that is a child of the bookstore. |
| `/bookstore/book[position()<3]` | Selects the first two book elements that are children of the bookstore element. |
| `//title[@lang]` | Selects all title elements that have an attribute named lang. |
| `//title[@lang='eng']` | Selects all title elements that have a lang attribute with the value eng. |
| `/bookstore/book[price>35.00]` | Selects all book elements of the bookstore element, and the value of the price element must be greater than 35.00. |
| `/bookstore/book[price>35.00]/title` | Selects all title elements of the book element in the bookstore element, and the value of the price element must be greater than 35.00. |

XPath also supports wildcard usage as shown below.

| path expression | result |
| -------------- | ---------------------------------- |
| `/bookstore/*` | Selects all children of the bookstore element. |
| `//*` | Selects all elements in the document. |
| `//title[@*]` | Selects all title elements with attributes. |

If you want to pick multiple nodes, you can use the method shown below.

| path expression | result |
| ---------------------------------- | -------------- -------------------------------------------------------- |
| `//book/title \| //book/price` | Selects all title and price elements of the book element. |
| `//title \| //price` | Selects all title and price elements in the document. |
| `/bookstore/book/title \| //price` | Selects all title elements that belong to the book element of the bookstore element, and all price elements in the document. |

> **Note**: The above example comes from the [XPath Tutorial](<https://www.runoob.com/xpath/xpath-tutorial.html>) on the "Rookie Tutorial" website. Interested readers can Read the original text yourself.

Of course, if you do not understand or are not familiar with the XPath syntax, you can view the XPath syntax of the element in the developer tools of the browser as shown below. XPath syntax for movie titles in .

![](https://gitee.com/jackfrued/mypic/raw/master/20210822093707.png)

Implementing XPath parsing requires the support of the third-party library `lxml`, you can use the following command to install `lxml`.


````Bash
pip install lxml
````

Next, we use XPath parsing to rewrite the previous code for obtaining the Top250 Douban movie, as shown below.

````Python
from lxml import etree
import requests

for page in range(1, 11):
    resp = requests.get(
        url=f'https://movie.douban.com/top250?start={(page - 1) * 25}',
        headers={'User-Agent': 'BaiduSpider'}
    )
    tree = etree.HTML(resp.text)
    # Extract movie title from page via XPath syntax
    title_spans = tree.xpath('//*[@id="content"]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]')
    # Extract movie ratings from page via XPath syntax
    rank_spans = tree.xpath('//*[@id="content"]/div/div[1]/ol/li[1]/div/div[2]/div[2]/div/span[2 ]')
    for title_span, rank_span in zip(title_spans, rank_spans):
        print(title_span.text, rank_span.text)
````

### CSS selector parsing

For developers familiar with CSS selectors and JavaScript, retrieving page elements through CSS selectors may be an easier option, because JavaScript running in the browser itself can access the `querySelector()` and `document` objects. The querySelectorAll()` method gets page elements based on CSS selectors. In Python, we can leverage the tripartite library `beautifulsoup4` or `pyquery` to do the same. Beautiful Soup can be used to parse HTML and XML documents, repair documents with errors such as unclosed tags, and encapsulate the operation of extracting data from pages by creating a tree structure in memory for the pages to be parsed. Beautiful Soup can be installed with the following command.

````Python
pip install beautifulsoup4
````

The following is the code rewritten using `bs4` to get the names of Douban movies Top250 movies.

````Python
import bs4
import requests

for page in range(1, 11):
    resp = requests.get(
        url=f'https://movie.douban.com/top250?start={(page - 1) * 25}',
        headers={'User-Agent': 'BaiduSpider'}
    )
    # Create BeautifulSoup object
    soup = bs4.BeautifulSoup(resp.text, 'lxml')
    # Extract span tag containing movie title from page via CSS selector
    title_spans = soup.select('div.info > div.hd > a > span:nth-child(1)')
    # Extract span tags containing movie ratings from the page via CSS selectors
    rank_spans = soup.select('div.info > div.bd > div > span.rating_num')
    for title_span, rank_span in zip(title_spans, rank_spans):
        print(title_span.text, rank_span.text)
````

For more knowledge about BeautifulSoup, you can refer to its [official document](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/).

### A brief summary

Below we make a simple comparison of the three analysis methods.

| Analysis method | Corresponding module | Speed ​​| Difficulty of use |
| -------------- | ---------------- | ------ | -------- |
| Regular Expression Parsing | `re` | Fast | Difficult |
| XPath parsing | `lxml` | fast | general |
| CSS selector parsing | `bs4` or `pyquery` | uncertain | simple |