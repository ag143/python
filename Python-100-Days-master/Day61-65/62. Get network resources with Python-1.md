## Get network data with Python

Network data collection is an area that the Python language is very good at. As we mentioned in the last lesson, the programs that implement network data collection are usually called web crawlers or spider programs. Even in the era of big data, data is still a flaw and shortcoming for small and medium-sized enterprises. Some data needs to be obtained through open or paid data interfaces, while other industry data and competitive data must be collected through network data. way to obtain. No matter which method is used to obtain network data resources, Python language is a very good choice, because Python's standard library and third-party library provide good support for network data collection.

### requests library

To get network data using Python, we recommend that you use a third-party library called `requests`, which we have actually used in previous courses. According to the explanation on the official website, `requests` is encapsulated based on the Python standard library, which simplifies the operation of accessing network resources through HTTP or HTTPS. We mentioned in class that HTTP is a request-response protocol, when we enter the correct [URL] in the browser (https://developer.mozilla.org/zh-CN/docs/Learn/Common_questions/What_is_a_URL) (often called a URL) and pressing Enter, we send a [Web Server](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_web_server) HTTP request, the server will give us an HTTP response after receiving the request. Open the "Developer Tools" menu in the Chrome browser and switch to the "Network" tab to see what the HTTP request and response look like, as shown in the figure below.

![](https://gitee.com/jackfrued/mypic/raw/master/20210822093434.png)

Through the `requests` library, we can let the Python program initiate a request to the web server like a browser, and receive the response returned by the server, from which we can extract the desired data. The web pages presented to us by the browser are written in [HTML](https://developer.mozilla.org/zh-CN/docs/Web/HTML). The browser is equivalent to an HTML interpreter environment. What we see The content of a web page is contained in HTML tags. After getting the HTML code, you can extract the content from the tag's attributes or tag body. The following example demonstrates how to get the HTML code of the web page. We obtained the code of the Sohu homepage through the `get` function of the `requests` library.

````Python
import requests

resp = requests.get('https://www.sohu.com/')
if resp.status_code == 200:
    print(resp.text)
````

> **Description**: The variable `resp` in the above code is a `Response` object (the type encapsulated by the `requests` library), the response status code can be obtained through the `status_code` attribute of the object, and the ` The text` attribute can help us get the HTML code of the page.

Since the `text` of the `Response` object is a string, we can use the knowledge of regular expressions discussed earlier to extract the headline and link of the news from the HTML code of the page, as shown below.

````Python
import re

import requests

pattern = re.compile(r'<a.*?href="(.*?)".*?title="(.*?)".*?>')
resp = requests.get('https://www.sohu.com/')
if resp.status_code == 200:
    all_matches = pattern.findall(resp.text)
    for href, title in all_matches:
        print(href)
        print(title)
````

In addition to text content, we can also use the `requests` library to fetch binary resources via URL. The following example demonstrates how to get the Baidu Logo and save it to a local file named `baidu.png`. You can right-click the Baidu Logo on Baidu's homepage and obtain the URL of the image through the "Copy Image Address" menu item.

````Python
import requests

resp = requests.get('https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png')
with open('baidu.png', 'wb') as file:
    file.write(resp.content)
````

> **Description**: The `content` property of the `Response` object can get the binary data of the server response.

The `requests` library is very easy to use and its functions are relatively powerful and complete. We will analyze the specific content for everyone in the process of using it. To unlock more knowledge about the `requests` library, you can read its [official documentation](https://docs.python-requests.org/zh_CN/latest/).

### Write crawler code

Next, we take "Douban Movie" as an example to explain how to write crawler code. According to the method provided above, we first use `requests` to get the HTML code of the web page, and then treat the entire code as a long string, so that we can use the regular expression capture group to extract the content we need from the string. The following code demonstrates how to get the names of the top 250 movies from [Douban Movie](https://movie.douban.com/). The page structure and corresponding code of [Douban Movie Top250](https://movie.douban.com/top250) are shown in the figure below. It can be seen that each page displays a total of 25 movies. If we want to obtain the Top250 data, we A total of 10 pages need to be accessed, and the corresponding address is <https://movie.douban.com/top250?start=xxx>, where `xxx` is the first page if it is `0`, if the value of `xxx` is `100`, then we can access the fifth page. To keep the code simple and easy to read, we only get the title and rating of the movie.

![](https://gitee.com/jackfrued/mypic/raw/master/20210822093447.png)

````Python
import random
import re
import time

import requests

for page in range(1, 11):
    resp = requests.get(
        url=f'https://movie.douban.com/top250?start={(page - 1) * 25}',
        # If the User-Agent in the HTTP request header is not set, Douban will detect that it is not a browser and block our request.
        # Set the value of User-Agent through the headers parameter of the get function. The specific value can be viewed in the browser's developer tools.
        # When visiting most websites with a crawler, disguising the crawler as a request from the browser is a very important step.
        headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}
    )
    # Obtain the span tag whose class attribute is title and whose tag body does not start with & through regular expression and extract the tag content with capturing group
    pattern1 = re.compile(r'<span class="title">([^&]*?)</span>')
    titles = pattern1.findall(resp.text)
    # Get the span tag whose class attribute is rating_num through regular expression and extract the tag content with capture group
    pattern2 = re.compile(r'<span class="rating_num".*?>(.*?)</span>')
    ranks = pattern2.findall(resp.text)
    # zip the two lists, loop through all movie titles and ratings
    for title, rank in zip(titles, ranks):
        print(title, rank)
    # Randomly sleep for 1-5 seconds to avoid crawling pages too frequently
    time.sleep(random.random() * 4 + 1)
````

> **Note**: By analyzing the robots protocol of Douban.com, we found that Douban.com does not refuse Baidu crawlers to obtain its data, so we can also disguise the crawlers as Baidu's crawlers, and use the `headers` of the `get` function. The parameter is modified to: `headers={'User-Agent': 'BaiduSpider'}`.

### Using IP Proxy

It is more important to write a crawler program to let the crawler program hide its identity. Many websites are disgusted with the crawler, because the crawler will consume a lot of their network bandwidth and create a lot of invalid traffic. To hide your identity, you usually need to use a commercial IP proxy (such as mushroom proxy, sesame proxy, fast proxy, etc.), so that the crawled website cannot obtain the real IP address of the source of the crawler, so it is impossible to simply pass the IP address. Ban bots.

The following takes [Mushroom Proxy](http://www.moguproxy.com/) as an example to explain the use of commercial IP proxy. First, you need to register an account on the website. After registering the account, you can [buy](http://www.moguproxy.com/buy) the corresponding package to obtain a commercial IP proxy. For commercial purposes, it is recommended that you buy an unlimited package, so that you can obtain enough proxy IP addresses according to actual needs; for learning purposes, you can purchase a package time package or decide according to your own needs. Mushroom proxy provides two ways to access the proxy, namely API private proxy and HTTP tunnel proxy. The former is to obtain the proxy server address by requesting the API interface of Mushroom proxy, and the latter is to directly use the unified entry (domain name provided by Mushroom proxy). ) to access.

<img src="https://gitee.com/jackfrued/mypic/raw/master/20210829080647.png" width="75%">

Below, we take the HTTP tunnel proxy as an example to explain the way to access the IP proxy. You can also directly refer to the code provided by the official website of the mushroom proxy to set the proxy for the crawler.

````Python
import requests

APP_KEY = 'Wnp******************************XFx'
PROXY_HOST = 'secondtransfer.moguproxy.com:9001'

for page in range(1, 11):
    resp = requests.get(
        url=f'https://movie.douban.com/top250?start={(page - 1) * 25}',
        # The authentication method of the proxy needs to be set in the HTTP request header
        headers={
            'Proxy-Authorization': f'Basic {APP_KEY}',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4'
        },
        # set proxy server
        proxies={
            'http': f'http://{PROXY_HOST}',
            'https': f'https://{PROXY_HOST}'
        },
        verify=False
    )
    pattern1 = re.compile(r'<span class="title">([^&]*?)</span>')
    titles = pattern1.findall(resp.text)
    pattern2 = re.compile(r'<span class="rating_num".*?>(.*?)</span>')
    ranks = pattern2.findall(resp.text)
    for title, rank in zip(titles, ranks):
        print(title, rank)
````

> **Note**: The above code needs to modify the `Appkey` value corresponding to the order created by `APP_KEY`, which can be viewed in the user order in the user center. Mushroom Proxy provides free API proxy and HTTP tunnel proxy trials, but the connection rate of the trial proxy cannot be guaranteed. It is recommended that you purchase a proxy service within your ability to pay to experience it.

### A brief summary

There are really many things that the Python language can do. As far as network data collection is concerned, Python is almost unique. A large number of enterprises and individuals are using Python to obtain the data they need from the Internet. This may also be your daily routine in the future. part of the work. In addition, although it is feasible to extract content from web pages by writing regular expressions, it is not easy to write a regular expression that can meet the needs, especially for beginners. In the next lesson, we will introduce two other methods for extracting data from pages. Although they may not be as good as regular expressions in terms of performance, they reduce the complexity of coding. I believe you will like it. their.
